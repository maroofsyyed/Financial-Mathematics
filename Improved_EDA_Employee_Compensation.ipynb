{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved EDA and Error Analysis for Employee Compensation Prediction\n",
    "\n",
    "This notebook provides an enhanced exploratory data analysis (EDA) with comprehensive error calculation and model evaluation techniques for employee compensation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for comprehensive analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, jarque_bera, normaltest, anderson\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, \n",
    "    mean_absolute_percentage_error, explained_variance_score\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Enhanced display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with enhanced error handling\n",
    "try:\n",
    "    df = pd.read_csv('employee_compensation 3.csv')\n",
    "    print(f\"✅ Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ File not found. Please ensure 'employee_compensation 3.csv' is in the current directory.\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    df = pd.DataFrame({\n",
    "        'Type_of_Year': np.random.choice(['Financial', 'Calendar'], n_samples),\n",
    "        'Year': np.random.choice([2013, 2014, 2015], n_samples),\n",
    "        'Emp_ID': range(1, n_samples + 1),\n",
    "        'Income': np.random.normal(75000, 25000, n_samples),\n",
    "        'Overtime': np.random.exponential(5000, n_samples),\n",
    "        'Other_Income': np.random.exponential(3000, n_samples),\n",
    "        'Total_Income': lambda x: x['Income'] + x['Overtime'] + x['Other_Income'],\n",
    "        'Retirement': lambda x: x['Total_Income'] * 0.15 + np.random.normal(0, 1000, n_samples),\n",
    "        'Health_Insurance': np.random.normal(8000, 2000, n_samples),\n",
    "        'Other_Benefits': np.random.normal(4000, 1500, n_samples),\n",
    "    })\n",
    "    df['Total_Benefits'] = df['Retirement'] + df['Health_Insurance'] + df['Other_Benefits']\n",
    "    df['Total_Reimbursement'] = df['Total_Income'] + df['Total_Benefits']\n",
    "    print(f\"✅ Sample data created: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n📊 Dataset Overview:\")\n",
    "print(df.info())\n",
    "print(\"\\n📈 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report\n",
    "    \"\"\"\n",
    "    print(\"🔍 COMPREHENSIVE DATA QUALITY REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"📊 Dataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"💾 Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percent\n",
    "    }).sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    print(\"\\n🚫 Missing Values Analysis:\")\n",
    "    if missing_df['Missing_Count'].sum() == 0:\n",
    "        print(\"✅ No missing values found!\")\n",
    "    else:\n",
    "        print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "    \n",
    "    # Data types analysis\n",
    "    print(\"\\n📋 Data Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Duplicate analysis\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\n🔄 Duplicate Rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Numerical columns analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\n🔢 Numerical Columns: {len(numeric_cols)}\")\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    print(f\"📝 Categorical Columns: {len(categorical_cols)}\")\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\n📊 Categorical Variables Cardinality:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"  {col}: {unique_count} unique values\")\n",
    "    \n",
    "    return missing_df, numeric_cols, categorical_cols\n",
    "\n",
    "missing_analysis, numeric_columns, categorical_columns = comprehensive_data_quality_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_statistical_summary(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Generate advanced statistical summary with distribution analysis\n",
    "    \"\"\"\n",
    "    print(\"📊 ADVANCED STATISTICAL SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    stats_summary = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Basic statistics\n",
    "        basic_stats = {\n",
    "            'Column': col,\n",
    "            'Count': len(data),\n",
    "            'Mean': data.mean(),\n",
    "            'Median': data.median(),\n",
    "            'Mode': data.mode().iloc[0] if len(data.mode()) > 0 else np.nan,\n",
    "            'Std': data.std(),\n",
    "            'Variance': data.var(),\n",
    "            'Min': data.min(),\n",
    "            'Max': data.max(),\n",
    "            'Range': data.max() - data.min(),\n",
    "            'Q1': data.quantile(0.25),\n",
    "            'Q3': data.quantile(0.75),\n",
    "            'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
    "            'Skewness': stats.skew(data),\n",
    "            'Kurtosis': stats.kurtosis(data),\n",
    "            'CV': data.std() / data.mean() if data.mean() != 0 else np.nan\n",
    "        }\n",
    "        \n",
    "        # Normality tests\n",
    "        try:\n",
    "            shapiro_stat, shapiro_p = shapiro(data.sample(min(5000, len(data))))\n",
    "            basic_stats['Shapiro_p'] = shapiro_p\n",
    "            basic_stats['Is_Normal_Shapiro'] = shapiro_p > 0.05\n",
    "        except:\n",
    "            basic_stats['Shapiro_p'] = np.nan\n",
    "            basic_stats['Is_Normal_Shapiro'] = False\n",
    "        \n",
    "        try:\n",
    "            jb_stat, jb_p = jarque_bera(data)\n",
    "            basic_stats['JB_p'] = jb_p\n",
    "            basic_stats['Is_Normal_JB'] = jb_p > 0.05\n",
    "        except:\n",
    "            basic_stats['JB_p'] = np.nan\n",
    "            basic_stats['Is_Normal_JB'] = False\n",
    "        \n",
    "        # Outlier detection using IQR method\n",
    "        Q1, Q3 = data.quantile(0.25), data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        basic_stats['Outliers_Count'] = len(outliers)\n",
    "        basic_stats['Outliers_Percentage'] = len(outliers) / len(data) * 100\n",
    "        \n",
    "        stats_summary.append(basic_stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_summary)\n",
    "    return stats_df\n",
    "\n",
    "# Generate advanced statistical summary\n",
    "advanced_stats = advanced_statistical_summary(df, numeric_columns)\n",
    "print(\"\\n📊 Advanced Statistical Summary:\")\n",
    "display(advanced_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Visualization Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualization_suite(df, numeric_cols, target_col='Total_Reimbursement'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization suite for EDA\n",
    "    \"\"\"\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # 1. Distribution Analysis\n",
    "    n_cols = min(4, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:len(axes)]):\n",
    "        # Histogram with KDE\n",
    "        sns.histplot(data=df, x=col, kde=True, ax=axes[i], alpha=0.7)\n",
    "        axes[i].axvline(df[col].mean(), color='red', linestyle='--', alpha=0.8, label=f'Mean: {df[col].mean():.2f}')\n",
    "        axes[i].axvline(df[col].median(), color='green', linestyle='--', alpha=0.8, label=f'Median: {df[col].median():.2f}')\n",
    "        axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('📊 Distribution Analysis of Numerical Variables', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Correlation Heatmap with Enhanced Features\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdYlBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.3f',\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    plt.title('🔗 Enhanced Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Box Plots for Outlier Detection\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:len(axes)]):\n",
    "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "        axes[i].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistical annotations\n",
    "        q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        outliers = df[(df[col] < q1 - 1.5*iqr) | (df[col] > q3 + 1.5*iqr)][col]\n",
    "        axes[i].text(0.02, 0.98, f'Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('📦 Outlier Detection via Box Plots', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Pairwise Relationships (focusing on target variable)\n",
    "    if target_col in numeric_cols:\n",
    "        other_cols = [col for col in numeric_cols if col != target_col][:6]  # Limit to 6 for readability\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(other_cols[:6]):\n",
    "            sns.scatterplot(data=df, x=col, y=target_col, ax=axes[i], alpha=0.6)\n",
    "            \n",
    "            # Add regression line\n",
    "            sns.regplot(data=df, x=col, y=target_col, ax=axes[i], \n",
    "                       scatter=False, color='red', line_kws={'linewidth': 2})\n",
    "            \n",
    "            # Calculate and display correlation\n",
    "            corr = df[col].corr(df[target_col])\n",
    "            axes[i].set_title(f'{col} vs {target_col}\\nCorrelation: {corr:.3f}', \n",
    "                             fontsize=11, fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(other_cols), 6):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'🎯 Relationships with Target Variable: {target_col}', \n",
    "                     fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "create_comprehensive_visualization_suite(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Model Building with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df, target_col='Total_Reimbursement'):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling with proper preprocessing\n",
    "    \"\"\"\n",
    "    # Select features (excluding target and ID columns)\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'Emp_ID']]\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Update feature columns after encoding\n",
    "    feature_cols = [col for col in df_processed.columns if col not in [target_col, 'Emp_ID']]\n",
    "    \n",
    "    X = df_processed[feature_cols]\n",
    "    y = df_processed[target_col]\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare data\n",
    "X, y, feature_names = prepare_data_for_modeling(df)\n",
    "print(f\"✅ Data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"📊 Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_evaluation(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple algorithms and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=1.0),\n",
    "        'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=random_state),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=random_state)\n",
    "    }\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    predictions = {}\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    print(\"🔄 Training and evaluating models...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n🤖 Training {name}...\")\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "        cv_rmse = -cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_root_mean_squared_error')\n",
    "        cv_mae = -cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        \n",
    "        # Fit model and make predictions\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Store predictions for later analysis\n",
    "        predictions[name] = {\n",
    "            'train_pred': y_pred_train,\n",
    "            'test_pred': y_pred_test,\n",
    "            'train_actual': y_train,\n",
    "            'test_actual': y_test\n",
    "        }\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        train_metrics = calculate_regression_metrics(y_train, y_pred_train)\n",
    "        test_metrics = calculate_regression_metrics(y_test, y_pred_test)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'Model': name,\n",
    "            'CV_R2_Mean': cv_scores.mean(),\n",
    "            'CV_R2_Std': cv_scores.std(),\n",
    "            'CV_RMSE_Mean': cv_rmse.mean(),\n",
    "            'CV_RMSE_Std': cv_rmse.std(),\n",
    "            'CV_MAE_Mean': cv_mae.mean(),\n",
    "            'CV_MAE_Std': cv_mae.std(),\n",
    "            'Train_R2': train_metrics['R2'],\n",
    "            'Test_R2': test_metrics['R2'],\n",
    "            'Train_RMSE': train_metrics['RMSE'],\n",
    "            'Test_RMSE': test_metrics['RMSE'],\n",
    "            'Train_MAE': train_metrics['MAE'],\n",
    "            'Test_MAE': test_metrics['MAE'],\n",
    "            'Train_MAPE': train_metrics['MAPE'],\n",
    "            'Test_MAPE': test_metrics['MAPE'],\n",
    "            'Overfitting_Score': train_metrics['R2'] - test_metrics['R2']\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  ✅ CV R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "        print(f\"  📊 Test R²: {test_metrics['R2']:.4f}\")\n",
    "        print(f\"  📉 Test RMSE: {test_metrics['RMSE']:.2f}\")\n",
    "        print(f\"  📈 Overfitting: {result['Overfitting_Score']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, predictions, (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive regression metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,\n",
    "        'Explained_Variance': explained_variance_score(y_true, y_pred),\n",
    "        'MSE': mean_squared_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Run comprehensive model evaluation\n",
    "model_results, model_predictions, data_splits = comprehensive_model_evaluation(X, y)\n",
    "\n",
    "print(\"\\n📊 COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "display(model_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Error Analysis and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_error_analysis(model_results, model_predictions, data_splits):\n",
    "    \"\"\"\n",
    "    Perform advanced error analysis and diagnostics\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = data_splits\n",
    "    \n",
    "    # 1. Model Performance Comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # R² Comparison\n",
    "    models = model_results['Model']\n",
    "    train_r2 = model_results['Train_R2']\n",
    "    test_r2 = model_results['Test_R2']\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x - width/2, train_r2, width, label='Train R²', alpha=0.8)\n",
    "    axes[0,0].bar(x + width/2, test_r2, width, label='Test R²', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Models')\n",
    "    axes[0,0].set_ylabel('R² Score')\n",
    "    axes[0,0].set_title('R² Score Comparison: Train vs Test')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMSE Comparison\n",
    "    train_rmse = model_results['Train_RMSE']\n",
    "    test_rmse = model_results['Test_RMSE']\n",
    "    \n",
    "    axes[0,1].bar(x - width/2, train_rmse, width, label='Train RMSE', alpha=0.8)\n",
    "    axes[0,1].bar(x + width/2, test_rmse, width, label='Test RMSE', alpha=0.8)\n",
    "    axes[0,1].set_xlabel('Models')\n",
    "    axes[0,1].set_ylabel('RMSE')\n",
    "    axes[0,1].set_title('RMSE Comparison: Train vs Test')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cross-Validation Performance\n",
    "    cv_r2_mean = model_results['CV_R2_Mean']\n",
    "    cv_r2_std = model_results['CV_R2_Std']\n",
    "    \n",
    "    axes[1,0].bar(x, cv_r2_mean, yerr=cv_r2_std, capsize=5, alpha=0.8)\n",
    "    axes[1,0].set_xlabel('Models')\n",
    "    axes[1,0].set_ylabel('CV R² Score')\n",
    "    axes[1,0].set_title('Cross-Validation R² Score (Mean ± Std)')\n",
    "    axes[1,0].set_xticks(x)\n",
    "    axes[1,0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting Analysis\n",
    "    overfitting_scores = model_results['Overfitting_Score']\n",
    "    colors = ['red' if score > 0.1 else 'orange' if score > 0.05 else 'green' for score in overfitting_scores]\n",
    "    \n",
    "    axes[1,1].bar(x, overfitting_scores, color=colors, alpha=0.8)\n",
    "    axes[1,1].axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Moderate Overfitting')\n",
    "    axes[1,1].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='High Overfitting')\n",
    "    axes[1,1].set_xlabel('Models')\n",
    "    axes[1,1].set_ylabel('Overfitting Score (Train R² - Test R²)')\n",
    "    axes[1,1].set_title('Overfitting Analysis')\n",
    "    axes[1,1].set_xticks(x)\n",
    "    axes[1,1].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('🔍 Advanced Model Performance Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Residual Analysis for Best Model\n",
    "    best_model_idx = model_results['Test_R2'].idxmax()\n",
    "    best_model_name = model_results.loc[best_model_idx, 'Model']\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
    "    print(f\"📊 Test R²: {model_results.loc[best_model_idx, 'Test_R2']:.4f}\")\n",
    "    print(f\"📉 Test RMSE: {model_results.loc[best_model_idx, 'Test_RMSE']:.2f}\")\n",
    "    \n",
    "    # Get predictions for best model\n",
    "    best_predictions = model_predictions[best_model_name]\n",
    "    y_test_pred = best_predictions['test_pred']\n",
    "    y_test_actual = best_predictions['test_actual']\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test_actual - y_test_pred\n",
    "    \n",
    "    # Residual Analysis Plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Actual vs Predicted\n",
    "    axes[0,0].scatter(y_test_actual, y_test_pred, alpha=0.6)\n",
    "    axes[0,0].plot([y_test_actual.min(), y_test_actual.max()], \n",
    "                   [y_test_actual.min(), y_test_actual.max()], 'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Actual Values')\n",
    "    axes[0,0].set_ylabel('Predicted Values')\n",
    "    axes[0,0].set_title('Actual vs Predicted Values')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R² annotation\n",
    "    r2_test = r2_score(y_test_actual, y_test_pred)\n",
    "    axes[0,0].text(0.05, 0.95, f'R² = {r2_test:.4f}', transform=axes[0,0].transAxes,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 2. Residuals vs Predicted\n",
    "    axes[0,1].scatter(y_test_pred, residuals, alpha=0.6)\n",
    "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Predicted Values')\n",
    "    axes[0,1].set_ylabel('Residuals')\n",
    "    axes[0,1].set_title('Residuals vs Predicted Values')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Distribution\n",
    "    sns.histplot(residuals, kde=True, ax=axes[0,2])\n",
    "    axes[0,2].axvline(residuals.mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {residuals.mean():.2f}')\n",
    "    axes[0,2].set_xlabel('Residuals')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "    axes[0,2].set_title('Distribution of Residuals')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q Plot for Residuals\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1,0])\n",
    "    axes[1,0].set_title('Q-Q Plot of Residuals')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Scale-Location Plot\n",
    "    standardized_residuals = np.sqrt(np.abs(residuals / residuals.std()))\n",
    "    axes[1,1].scatter(y_test_pred, standardized_residuals, alpha=0.6)\n",
    "    axes[1,1].set_xlabel('Predicted Values')\n",
    "    axes[1,1].set_ylabel('√|Standardized Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Error Distribution by Prediction Range\n",
    "    pred_ranges = pd.cut(y_test_pred, bins=5, labels=['Low', 'Low-Med', 'Medium', 'Med-High', 'High'])\n",
    "    error_by_range = pd.DataFrame({'Range': pred_ranges, 'Absolute_Error': np.abs(residuals)})\n",
    "    sns.boxplot(data=error_by_range, x='Range', y='Absolute_Error', ax=axes[1,2])\n",
    "    axes[1,2].set_title('Error Distribution by Prediction Range')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'🔬 Residual Analysis: {best_model_name}', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model_name, residuals\n",
    "\n",
    "# Perform advanced error analysis\n",
    "best_model, residuals = advanced_error_analysis(model_results, model_predictions, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Tests and Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_diagnostics(residuals, X_test, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Perform comprehensive statistical tests and diagnostics\n",
    "    \"\"\"\n",
    "    print(\"🧪 COMPREHENSIVE MODEL DIAGNOSTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    diagnostics_results = {}\n",
    "    \n",
    "    # 1. Normality Tests for Residuals\n",
    "    print(\"\\n📊 NORMALITY TESTS FOR RESIDUALS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Shapiro-Wilk Test\n",
    "    if len(residuals) <= 5000:\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "        print(f\"Shapiro-Wilk Test:\")\n",
    "        print(f\"  Statistic: {shapiro_stat:.6f}\")\n",
    "        print(f\"  p-value: {shapiro_p:.6f}\")\n",
    "        print(f\"  Normal: {'✅ Yes' if shapiro_p > 0.05 else '❌ No'}\")\n",
    "        diagnostics_results['shapiro_p'] = shapiro_p\n",
    "    else:\n",
    "        print(\"Shapiro-Wilk Test: Skipped (sample too large)\")\n",
    "    \n",
    "    # Jarque-Bera Test\n",
    "    jb_stat, jb_p = jarque_bera(residuals)\n",
    "    print(f\"\\nJarque-Bera Test:\")\n",
    "    print(f\"  Statistic: {jb_stat:.6f}\")\n",
    "    print(f\"  p-value: {jb_p:.6f}\")\n",
    "    print(f\"  Normal: {'✅ Yes' if jb_p > 0.05 else '❌ No'}\")\n",
    "    diagnostics_results['jb_p'] = jb_p\n",
    "    \n",
    "    # Anderson-Darling Test\n",
    "    ad_stat, ad_critical, ad_significance = anderson(residuals, dist='norm')\n",
    "    print(f\"\\nAnderson-Darling Test:\")\n",
    "    print(f\"  Statistic: {ad_stat:.6f}\")\n",
    "    print(f\"  Critical Values: {ad_critical}\")\n",
    "    print(f\"  Significance Levels: {ad_significance}%\")\n",
    "    normal_ad = ad_stat < ad_critical[2]  # 5% significance level\n",
    "    print(f\"  Normal (5% level): {'✅ Yes' if normal_ad else '❌ No'}\")\n",
    "    diagnostics_results['anderson_normal'] = normal_ad\n",
    "    \n",
    "    # 2. Homoscedasticity Tests\n",
    "    print(\"\\n🎯 HOMOSCEDASTICITY TESTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Prepare data for heteroscedasticity tests\n",
    "    X_with_const = sm.add_constant(X_test)\n",
    "    \n",
    "    try:\n",
    "        # Breusch-Pagan Test\n",
    "        bp_stat, bp_p, bp_f_stat, bp_f_p = het_breuschpagan(residuals, X_with_const)\n",
    "        print(f\"Breusch-Pagan Test:\")\n",
    "        print(f\"  LM Statistic: {bp_stat:.6f}\")\n",
    "        print(f\"  p-value: {bp_p:.6f}\")\n",
    "        print(f\"  Homoscedastic: {'✅ Yes' if bp_p > 0.05 else '❌ No'}\")\n",
    "        diagnostics_results['bp_p'] = bp_p\n",
    "        \n",
    "        # White Test\n",
    "        white_stat, white_p, white_f_stat, white_f_p = het_white(residuals, X_with_const)\n",
    "        print(f\"\\nWhite Test:\")\n",
    "        print(f\"  LM Statistic: {white_stat:.6f}\")\n",
    "        print(f\"  p-value: {white_p:.6f}\")\n",
    "        print(f\"  Homoscedastic: {'✅ Yes' if white_p > 0.05 else '❌ No'}\")\n",
    "        diagnostics_results['white_p'] = white_p\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Heteroscedasticity tests failed: {e}\")\n",
    "    \n",
    "    # 3. Error Distribution Analysis\n",
    "    print(\"\\n📈 ERROR DISTRIBUTION ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    abs_errors = np.abs(residuals)\n",
    "    squared_errors = residuals ** 2\n",
    "    \n",
    "    print(f\"Mean Absolute Error: {abs_errors.mean():.4f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(abs_errors):.4f}\")\n",
    "    print(f\"90th Percentile Error: {np.percentile(abs_errors, 90):.4f}\")\n",
    "    print(f\"95th Percentile Error: {np.percentile(abs_errors, 95):.4f}\")\n",
    "    print(f\"99th Percentile Error: {np.percentile(abs_errors, 99):.4f}\")\n",
    "    \n",
    "    # Error percentiles\n",
    "    error_percentiles = [50, 75, 90, 95, 99]\n",
    "    print(f\"\\nError Percentiles:\")\n",
    "    for p in error_percentiles:\n",
    "        print(f\"  {p}th percentile: {np.percentile(abs_errors, p):.4f}\")\n",
    "    \n",
    "    # 4. Prediction Intervals\n",
    "    print(\"\\n🎯 PREDICTION INTERVALS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    residual_std = residuals.std()\n",
    "    \n",
    "    # Calculate prediction intervals (assuming normal distribution)\n",
    "    confidence_levels = [0.68, 0.95, 0.99]\n",
    "    z_scores = [1.0, 1.96, 2.576]\n",
    "    \n",
    "    for conf, z in zip(confidence_levels, z_scores):\n",
    "        interval_width = z * residual_std\n",
    "        coverage = np.mean(np.abs(residuals) <= interval_width) * 100\n",
    "        print(f\"{conf*100:.0f}% Prediction Interval:\")\n",
    "        print(f\"  Width: ±{interval_width:.4f}\")\n",
    "        print(f\"  Actual Coverage: {coverage:.1f}%\")\n",
    "        print(f\"  Expected Coverage: {conf*100:.0f}%\")\n",
    "        print()\n",
    "    \n",
    "    # 5. Model Stability Analysis\n",
    "    print(\"🔄 MODEL STABILITY ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calculate metrics by prediction quantiles\n",
    "    pred_quantiles = pd.qcut(y_pred, q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "    stability_analysis = pd.DataFrame({\n",
    "        'Quantile': pred_quantiles,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Residual': residuals,\n",
    "        'Abs_Error': abs_errors\n",
    "    })\n",
    "    \n",
    "    stability_summary = stability_analysis.groupby('Quantile').agg({\n",
    "        'Abs_Error': ['mean', 'std', 'median'],\n",
    "        'Residual': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"Performance by Prediction Quantiles:\")\n",
    "    print(stability_summary)\n",
    "    \n",
    "    return diagnostics_results, stability_summary\n",
    "\n",
    "# Get predictions for the best model\n",
    "best_predictions = model_predictions[best_model]\n",
    "y_test_pred = best_predictions['test_pred']\n",
    "y_test_actual = best_predictions['test_actual']\n",
    "\n",
    "# Perform comprehensive diagnostics\n",
    "X_train, X_test, y_train, y_test = data_splits\n",
    "diagnostics, stability = comprehensive_model_diagnostics(residuals, X_test, y_test_actual, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance and Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(X_train, y_train, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    print(\"🎯 FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Random Forest Feature Importance\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # 2. Gradient Boosting Feature Importance\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    gb_importance = gb_model.feature_importances_\n",
    "    \n",
    "    # 3. Linear Regression Coefficients (absolute values)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    lr_importance = np.abs(lr_model.coef_)\n",
    "    lr_importance = lr_importance / lr_importance.sum()  # Normalize\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Random_Forest': rf_importance,\n",
    "        'Gradient_Boosting': gb_importance,\n",
    "        'Linear_Regression': lr_importance\n",
    "    })\n",
    "    \n",
    "    # Calculate average importance\n",
    "    importance_df['Average_Importance'] = importance_df[['Random_Forest', 'Gradient_Boosting', 'Linear_Regression']].mean(axis=1)\n",
    "    importance_df = importance_df.sort_values('Average_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n📊 Feature Importance Rankings:\")\n",
    "    display(importance_df.round(4))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Random Forest Importance\n",
    "    top_features_rf = importance_df.nlargest(10, 'Random_Forest')\n",
    "    axes[0,0].barh(range(len(top_features_rf)), top_features_rf['Random_Forest'])\n",
    "    axes[0,0].set_yticks(range(len(top_features_rf)))\n",
    "    axes[0,0].set_yticklabels(top_features_rf['Feature'])\n",
    "    axes[0,0].set_title('Random Forest Feature Importance')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient Boosting Importance\n",
    "    top_features_gb = importance_df.nlargest(10, 'Gradient_Boosting')\n",
    "    axes[0,1].barh(range(len(top_features_gb)), top_features_gb['Gradient_Boosting'])\n",
    "    axes[0,1].set_yticks(range(len(top_features_gb)))\n",
    "    axes[0,1].set_yticklabels(top_features_gb['Feature'])\n",
    "    axes[0,1].set_title('Gradient Boosting Feature Importance')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Linear Regression Coefficients\n",
    "    top_features_lr = importance_df.nlargest(10, 'Linear_Regression')\n",
    "    axes[1,0].barh(range(len(top_features_lr)), top_features_lr['Linear_Regression'])\n",
    "    axes[1,0].set_yticks(range(len(top_features_lr)))\n",
    "    axes[1,0].set_yticklabels(top_features_lr['Feature'])\n",
    "    axes[1,0].set_title('Linear Regression Coefficient Importance')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average Importance\n",
    "    top_features_avg = importance_df.nlargest(10, 'Average_Importance')\n",
    "    axes[1,1].barh(range(len(top_features_avg)), top_features_avg['Average_Importance'])\n",
    "    axes[1,1].set_yticks(range(len(top_features_avg)))\n",
    "    axes[1,1].set_yticklabels(top_features_avg['Feature'])\n",
    "    axes[1,1].set_title('Average Feature Importance')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('🎯 Feature Importance Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(data_splits[0], data_splits[2], feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Recommendations and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_recommendations(model_results, diagnostics, feature_importance, best_model):\n",
    "    \"\"\"\n",
    "    Generate final recommendations and summary\n",
    "    \"\"\"\n",
    "    print(\"🎯 FINAL MODEL RECOMMENDATIONS & SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Best model summary\n",
    "    best_model_row = model_results[model_results['Model'] == best_model].iloc[0]\n",
    "    \n",
    "    print(f\"\\n🏆 BEST MODEL: {best_model}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"📊 Test R²: {best_model_row['Test_R2']:.4f}\")\n",
    "    print(f\"📉 Test RMSE: {best_model_row['Test_RMSE']:.2f}\")\n",
    "    print(f\"📈 Test MAE: {best_model_row['Test_MAE']:.2f}\")\n",
    "    print(f\"🎯 Test MAPE: {best_model_row['Test_MAPE']:.2f}%\")\n",
    "    print(f\"🔄 CV R² (Mean±Std): {best_model_row['CV_R2_Mean']:.4f}±{best_model_row['CV_R2_Std']:.4f}\")\n",
    "    print(f\"⚠️  Overfitting Score: {best_model_row['Overfitting_Score']:.4f}\")\n",
    "    \n",
    "    # Model ranking\n",
    "    print(\"\\n📈 MODEL RANKING (by Test R²):\")\n",
    "    print(\"-\" * 35)\n",
    "    ranking = model_results.sort_values('Test_R2', ascending=False)[['Model', 'Test_R2', 'Test_RMSE', 'Overfitting_Score']]\n",
    "    for i, (_, row) in enumerate(ranking.iterrows(), 1):\n",
    "        overfitting_status = \"🔴 High\" if row['Overfitting_Score'] > 0.1 else \"🟡 Moderate\" if row['Overfitting_Score'] > 0.05 else \"🟢 Low\"\n",
    "        print(f\"{i}. {row['Model']}: R²={row['Test_R2']:.4f}, RMSE={row['Test_RMSE']:.2f}, Overfitting={overfitting_status}\")\n",
    "    \n",
    "    # Top features\n",
    "    print(\"\\n🎯 TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "    print(\"-\" * 40)\n",
    "    top_features = feature_importance.head(5)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"{i}. {row['Feature']}: {row['Average_Importance']:.4f}\")\n",
    "    \n",
    "    # Model diagnostics summary\n",
    "    print(\"\\n🧪 MODEL DIAGNOSTICS SUMMARY:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    if 'jb_p' in diagnostics:\n",
    "        normality_status = \"✅ Normal\" if diagnostics['jb_p'] > 0.05 else \"❌ Non-normal\"\n",
    "        print(f\"Residual Normality (Jarque-Bera): {normality_status} (p={diagnostics['jb_p']:.4f})\")\n",
    "    \n",
    "    if 'bp_p' in diagnostics:\n",
    "        homoscedasticity_status = \"✅ Homoscedastic\" if diagnostics['bp_p'] > 0.05 else \"❌ Heteroscedastic\"\n",
    "        print(f\"Homoscedasticity (Breusch-Pagan): {homoscedasticity_status} (p={diagnostics['bp_p']:.4f})\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n💡 RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Performance-based recommendations\n",
    "    if best_model_row['Test_R2'] > 0.9:\n",
    "        print(\"✅ Excellent model performance (R² > 0.9)\")\n",
    "    elif best_model_row['Test_R2'] > 0.8:\n",
    "        print(\"✅ Good model performance (R² > 0.8)\")\n",
    "    elif best_model_row['Test_R2'] > 0.7:\n",
    "        print(\"⚠️  Moderate model performance (R² > 0.7) - consider feature engineering\")\n",
    "    else:\n",
    "        print(\"❌ Poor model performance (R² < 0.7) - significant improvements needed\")\n",
    "    \n",
    "    # Overfitting recommendations\n",
    "    if best_model_row['Overfitting_Score'] > 0.1:\n",
    "        print(\"🔴 High overfitting detected - consider regularization or more data\")\n",
    "    elif best_model_row['Overfitting_Score'] > 0.05:\n",
    "        print(\"🟡 Moderate overfitting - monitor model generalization\")\n",
    "    else:\n",
    "        print(\"🟢 Low overfitting - good generalization\")\n",
    "    \n",
    "    # Feature recommendations\n",
    "    if len(feature_importance) > 10:\n",
    "        low_importance_features = feature_importance[feature_importance['Average_Importance'] < 0.01]\n",
    "        if len(low_importance_features) > 0:\n",
    "            print(f\"🔧 Consider removing {len(low_importance_features)} low-importance features\")\n",
    "    \n",
    "    # Model-specific recommendations\n",
    "    if best_model in ['Random Forest', 'Gradient Boosting']:\n",
    "        print(\"🌳 Tree-based model selected - good for non-linear relationships\")\n",
    "        print(\"💡 Consider hyperparameter tuning for further improvement\")\n",
    "    elif 'Regression' in best_model:\n",
    "        print(\"📈 Linear model selected - interpretable but may miss non-linear patterns\")\n",
    "        print(\"💡 Consider polynomial features or interaction terms\")\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    print(\"\\n🔧 NEXT STEPS FOR IMPROVEMENT:\")\n",
    "    print(\"-\" * 35)\n",
    "    print(\"1. 📊 Collect more data if possible\")\n",
    "    print(\"2. 🔧 Engineer new features based on domain knowledge\")\n",
    "    print(\"3. 🎯 Perform hyperparameter tuning\")\n",
    "    print(\"4. 🔄 Try ensemble methods combining multiple models\")\n",
    "    print(\"5. 📈 Monitor model performance over time\")\n",
    "    print(\"6. 🧪 Validate model on new, unseen data\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_r2': best_model_row['Test_R2'],\n",
    "        'best_rmse': best_model_row['Test_RMSE'],\n",
    "        'overfitting_score': best_model_row['Overfitting_Score'],\n",
    "        'top_features': top_features['Feature'].tolist()\n",
    "    }\n",
    "\n",
    "# Generate final recommendations\n",
    "final_summary = generate_final_recommendations(model_results, diagnostics, feature_importance, best_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 ANALYSIS COMPLETE!\")\n",
    "print(\"📊 This comprehensive EDA provides enhanced error analysis and model evaluation.\")\n",
    "print(\"💡 Use the insights above to improve your model further.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results and Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results(model_results, feature_importance, diagnostics, final_summary):\n",
    "    \"\"\"\n",
    "    Export analysis results to files\n",
    "    \"\"\"\n",
    "    print(\"💾 EXPORTING ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Export model comparison results\n",
    "        model_results.to_csv('model_comparison_results.csv', index=False)\n",
    "        print(\"✅ Model comparison results exported to 'model_comparison_results.csv'\")\n",
    "        \n",
    "        # Export feature importance\n",
    "        feature_importance.to_csv('feature_importance_analysis.csv', index=False)\n",
    "        print(\"✅ Feature importance analysis exported to 'feature_importance_analysis.csv'\")\n",
    "        \n",
    "        # Export summary report\n",
    "        with open('model_analysis_summary.txt', 'w') as f:\n",
    "            f.write(\"EMPLOYEE COMPENSATION MODEL ANALYSIS SUMMARY\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Best Model: {final_summary['best_model']}\\n\")\n",
    "            f.write(f\"Test R²: {final_summary['best_r2']:.4f}\\n\")\n",
    "            f.write(f\"Test RMSE: {final_summary['best_rmse']:.2f}\\n\")\n",
    "            f.write(f\"Overfitting Score: {final_summary['overfitting_score']:.4f}\\n\\n\")\n",
    "            f.write(\"Top 5 Features:\\n\")\n",
    "            for i, feature in enumerate(final_summary['top_features'], 1):\n",
    "                f.write(f\"{i}. {feature}\\n\")\n",
    "        \n",
    "        print(\"✅ Summary report exported to 'model_analysis_summary.txt'\")\n",
    "        \n",
    "        print(\"\\n📁 All results have been exported successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error exporting results: {e}\")\n",
    "\n",
    "# Export results\n",
    "export_analysis_results(model_results, feature_importance, diagnostics, final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}