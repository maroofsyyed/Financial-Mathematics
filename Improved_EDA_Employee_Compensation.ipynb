{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved EDA and Error Analysis for Employee Compensation Prediction\n",
    "\n",
    "This notebook provides an enhanced exploratory data analysis (EDA) with comprehensive error calculation and model evaluation techniques for employee compensation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enhanced Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports for comprehensive analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, jarque_bera, normaltest, anderson\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_white\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score, \n",
    "    mean_absolute_percentage_error, explained_variance_score\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Enhanced display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.6f}'.format)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with enhanced error handling\n",
    "try:\n",
    "    df = pd.read_csv('employee_compensation 3.csv')\n",
    "    print(f\"‚úÖ Data loaded successfully: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found. Please ensure 'employee_compensation 3.csv' is in the current directory.\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 5000\n",
    "    df = pd.DataFrame({\n",
    "        'Type_of_Year': np.random.choice(['Financial', 'Calendar'], n_samples),\n",
    "        'Year': np.random.choice([2013, 2014, 2015], n_samples),\n",
    "        'Emp_ID': range(1, n_samples + 1),\n",
    "        'Income': np.random.normal(75000, 25000, n_samples),\n",
    "        'Overtime': np.random.exponential(5000, n_samples),\n",
    "        'Other_Income': np.random.exponential(3000, n_samples),\n",
    "        'Total_Income': lambda x: x['Income'] + x['Overtime'] + x['Other_Income'],\n",
    "        'Retirement': lambda x: x['Total_Income'] * 0.15 + np.random.normal(0, 1000, n_samples),\n",
    "        'Health_Insurance': np.random.normal(8000, 2000, n_samples),\n",
    "        'Other_Benefits': np.random.normal(4000, 1500, n_samples),\n",
    "    })\n",
    "    df['Total_Benefits'] = df['Retirement'] + df['Health_Insurance'] + df['Other_Benefits']\n",
    "    df['Total_Reimbursement'] = df['Total_Income'] + df['Total_Benefits']\n",
    "    print(f\"‚úÖ Sample data created: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(df.info())\n",
    "print(\"\\nüìà First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive data quality report\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE DATA QUALITY REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"üìä Dataset Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"üíæ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values analysis\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing_data,\n",
    "        'Missing_Percentage': missing_percent\n",
    "    }).sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    print(\"\\nüö´ Missing Values Analysis:\")\n",
    "    if missing_df['Missing_Count'].sum() == 0:\n",
    "        print(\"‚úÖ No missing values found!\")\n",
    "    else:\n",
    "        print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "    \n",
    "    # Data types analysis\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "    \n",
    "    # Duplicate analysis\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate Rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Numerical columns analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    print(f\"\\nüî¢ Numerical Columns: {len(numeric_cols)}\")\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    print(f\"üìù Categorical Columns: {len(categorical_cols)}\")\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\nüìä Categorical Variables Cardinality:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"  {col}: {unique_count} unique values\")\n",
    "    \n",
    "    return missing_df, numeric_cols, categorical_cols\n",
    "\n",
    "missing_analysis, numeric_columns, categorical_columns = comprehensive_data_quality_report(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_statistical_summary(df, numeric_cols):\n",
    "    \"\"\"\n",
    "    Generate advanced statistical summary with distribution analysis\n",
    "    \"\"\"\n",
    "    print(\"üìä ADVANCED STATISTICAL SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    stats_summary = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        # Basic statistics\n",
    "        basic_stats = {\n",
    "            'Column': col,\n",
    "            'Count': len(data),\n",
    "            'Mean': data.mean(),\n",
    "            'Median': data.median(),\n",
    "            'Mode': data.mode().iloc[0] if len(data.mode()) > 0 else np.nan,\n",
    "            'Std': data.std(),\n",
    "            'Variance': data.var(),\n",
    "            'Min': data.min(),\n",
    "            'Max': data.max(),\n",
    "            'Range': data.max() - data.min(),\n",
    "            'Q1': data.quantile(0.25),\n",
    "            'Q3': data.quantile(0.75),\n",
    "            'IQR': data.quantile(0.75) - data.quantile(0.25),\n",
    "            'Skewness': stats.skew(data),\n",
    "            'Kurtosis': stats.kurtosis(data),\n",
    "            'CV': data.std() / data.mean() if data.mean() != 0 else np.nan\n",
    "        }\n",
    "        \n",
    "        # Normality tests\n",
    "        try:\n",
    "            shapiro_stat, shapiro_p = shapiro(data.sample(min(5000, len(data))))\n",
    "            basic_stats['Shapiro_p'] = shapiro_p\n",
    "            basic_stats['Is_Normal_Shapiro'] = shapiro_p > 0.05\n",
    "        except:\n",
    "            basic_stats['Shapiro_p'] = np.nan\n",
    "            basic_stats['Is_Normal_Shapiro'] = False\n",
    "        \n",
    "        try:\n",
    "            jb_stat, jb_p = jarque_bera(data)\n",
    "            basic_stats['JB_p'] = jb_p\n",
    "            basic_stats['Is_Normal_JB'] = jb_p > 0.05\n",
    "        except:\n",
    "            basic_stats['JB_p'] = np.nan\n",
    "            basic_stats['Is_Normal_JB'] = False\n",
    "        \n",
    "        # Outlier detection using IQR method\n",
    "        Q1, Q3 = data.quantile(0.25), data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "        basic_stats['Outliers_Count'] = len(outliers)\n",
    "        basic_stats['Outliers_Percentage'] = len(outliers) / len(data) * 100\n",
    "        \n",
    "        stats_summary.append(basic_stats)\n",
    "    \n",
    "    stats_df = pd.DataFrame(stats_summary)\n",
    "    return stats_df\n",
    "\n",
    "# Generate advanced statistical summary\n",
    "advanced_stats = advanced_statistical_summary(df, numeric_columns)\n",
    "print(\"\\nüìä Advanced Statistical Summary:\")\n",
    "display(advanced_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Visualization Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualization_suite(df, numeric_cols, target_col='Total_Reimbursement'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization suite for EDA\n",
    "    \"\"\"\n",
    "    # Set up the plotting style\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    \n",
    "    # 1. Distribution Analysis\n",
    "    n_cols = min(4, len(numeric_cols))\n",
    "    n_rows = (len(numeric_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:len(axes)]):\n",
    "        # Histogram with KDE\n",
    "        sns.histplot(data=df, x=col, kde=True, ax=axes[i], alpha=0.7)\n",
    "        axes[i].axvline(df[col].mean(), color='red', linestyle='--', alpha=0.8, label=f'Mean: {df[col].mean():.2f}')\n",
    "        axes[i].axvline(df[col].median(), color='green', linestyle='--', alpha=0.8, label=f'Median: {df[col].median():.2f}')\n",
    "        axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('üìä Distribution Analysis of Numerical Variables', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Correlation Heatmap with Enhanced Features\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='RdYlBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.3f',\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    plt.title('üîó Enhanced Correlation Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Box Plots for Outlier Detection\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "    axes = axes.flatten() if n_rows > 1 else [axes] if n_rows == 1 else axes\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:len(axes)]):\n",
    "        sns.boxplot(data=df, y=col, ax=axes[i])\n",
    "        axes[i].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add statistical annotations\n",
    "        q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        outliers = df[(df[col] < q1 - 1.5*iqr) | (df[col] > q3 + 1.5*iqr)][col]\n",
    "        axes[i].text(0.02, 0.98, f'Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('üì¶ Outlier Detection via Box Plots', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Pairwise Relationships (focusing on target variable)\n",
    "    if target_col in numeric_cols:\n",
    "        other_cols = [col for col in numeric_cols if col != target_col][:6]  # Limit to 6 for readability\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, col in enumerate(other_cols[:6]):\n",
    "            sns.scatterplot(data=df, x=col, y=target_col, ax=axes[i], alpha=0.6)\n",
    "            \n",
    "            # Add regression line\n",
    "            sns.regplot(data=df, x=col, y=target_col, ax=axes[i], \n",
    "                       scatter=False, color='red', line_kws={'linewidth': 2})\n",
    "            \n",
    "            # Calculate and display correlation\n",
    "            corr = df[col].corr(df[target_col])\n",
    "            axes[i].set_title(f'{col} vs {target_col}\\nCorrelation: {corr:.3f}', \n",
    "                             fontsize=11, fontweight='bold')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(other_cols), 6):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'üéØ Relationships with Target Variable: {target_col}', \n",
    "                     fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "create_comprehensive_visualization_suite(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Model Building with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_modeling(df, target_col='Total_Reimbursement'):\n",
    "    \"\"\"\n",
    "    Prepare data for modeling with proper preprocessing\n",
    "    \"\"\"\n",
    "    # Select features (excluding target and ID columns)\n",
    "    feature_cols = [col for col in df.columns if col not in [target_col, 'Emp_ID']]\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        df_processed = pd.get_dummies(df_processed, columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Update feature columns after encoding\n",
    "    feature_cols = [col for col in df_processed.columns if col not in [target_col, 'Emp_ID']]\n",
    "    \n",
    "    X = df_processed[feature_cols]\n",
    "    y = df_processed[target_col]\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare data\n",
    "X, y, feature_names = prepare_data_for_modeling(df)\n",
    "print(f\"‚úÖ Data prepared: {X.shape[0]} samples, {X.shape[1]} features\")\n",
    "print(f\"üìä Features: {feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_evaluation(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple algorithms and metrics\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Lasso Regression': Lasso(alpha=1.0),\n",
    "        'Elastic Net': ElasticNet(alpha=1.0, l1_ratio=0.5),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=random_state),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=random_state)\n",
    "    }\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = []\n",
    "    predictions = {}\n",
    "    \n",
    "    # Cross-validation setup\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    print(\"üîÑ Training and evaluating models...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nü§ñ Training {name}...\")\n",
    "        \n",
    "        # Cross-validation scores\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "        cv_rmse = -cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_root_mean_squared_error')\n",
    "        cv_mae = -cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')\n",
    "        \n",
    "        # Fit model and make predictions\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        \n",
    "        # Store predictions for later analysis\n",
    "        predictions[name] = {\n",
    "            'train_pred': y_pred_train,\n",
    "            'test_pred': y_pred_test,\n",
    "            'train_actual': y_train,\n",
    "            'test_actual': y_test\n",
    "        }\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        train_metrics = calculate_regression_metrics(y_train, y_pred_train)\n",
    "        test_metrics = calculate_regression_metrics(y_test, y_pred_test)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'Model': name,\n",
    "            'CV_R2_Mean': cv_scores.mean(),\n",
    "            'CV_R2_Std': cv_scores.std(),\n",
    "            'CV_RMSE_Mean': cv_rmse.mean(),\n",
    "            'CV_RMSE_Std': cv_rmse.std(),\n",
    "            'CV_MAE_Mean': cv_mae.mean(),\n",
    "            'CV_MAE_Std': cv_mae.std(),\n",
    "            'Train_R2': train_metrics['R2'],\n",
    "            'Test_R2': test_metrics['R2'],\n",
    "            'Train_RMSE': train_metrics['RMSE'],\n",
    "            'Test_RMSE': test_metrics['RMSE'],\n",
    "            'Train_MAE': train_metrics['MAE'],\n",
    "            'Test_MAE': test_metrics['MAE'],\n",
    "            'Train_MAPE': train_metrics['MAPE'],\n",
    "            'Test_MAPE': test_metrics['MAPE'],\n",
    "            'Overfitting_Score': train_metrics['R2'] - test_metrics['R2']\n",
    "        }\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  ‚úÖ CV R¬≤: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
    "        print(f\"  üìä Test R¬≤: {test_metrics['R2']:.4f}\")\n",
    "        print(f\"  üìâ Test RMSE: {test_metrics['RMSE']:.2f}\")\n",
    "        print(f\"  üìà Overfitting: {result['Overfitting_Score']:.4f}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df, predictions, (X_train, X_test, y_train, y_test)\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive regression metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'R2': r2_score(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,\n",
    "        'Explained_Variance': explained_variance_score(y_true, y_pred),\n",
    "        'MSE': mean_squared_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "# Run comprehensive model evaluation\n",
    "model_results, model_predictions, data_splits = comprehensive_model_evaluation(X, y)\n",
    "\n",
    "print(\"\\nüìä COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "display(model_results.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Error Analysis and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_error_analysis(model_results, model_predictions, data_splits):\n",
    "    \"\"\"\n",
    "    Perform advanced error analysis and diagnostics\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = data_splits\n",
    "    \n",
    "    # 1. Model Performance Comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # R¬≤ Comparison\n",
    "    models = model_results['Model']\n",
    "    train_r2 = model_results['Train_R2']\n",
    "    test_r2 = model_results['Test_R2']\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x - width/2, train_r2, width, label='Train R¬≤', alpha=0.8)\n",
    "    axes[0,0].bar(x + width/2, test_r2, width, label='Test R¬≤', alpha=0.8)\n",
    "    axes[0,0].set_xlabel('Models')\n",
    "    axes[0,0].set_ylabel('R¬≤ Score')\n",
    "    axes[0,0].set_title('R¬≤ Score Comparison: Train vs Test')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMSE Comparison\n",
    "    train_rmse = model_results['Train_RMSE']\n",
    "    test_rmse = model_results['Test_RMSE']\n",
    "    \n",
    "    axes[0,1].bar(x - width/2, train_rmse, width, label='Train RMSE', alpha=0.8)\n",
    "    axes[0,1].bar(x + width/2, test_rmse, width, label='Test RMSE', alpha=0.8)\n",
    "    axes[0,1].set_xlabel('Models')\n",
    "    axes[0,1].set_ylabel('RMSE')\n",
    "    axes[0,1].set_title('RMSE Comparison: Train vs Test')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cross-Validation Performance\n",
    "    cv_r2_mean = model_results['CV_R2_Mean']\n",
    "    cv_r2_std = model_results['CV_R2_Std']\n",
    "    \n",
    "    axes[1,0].bar(x, cv_r2_mean, yerr=cv_r2_std, capsize=5, alpha=0.8)\n",
    "    axes[1,0].set_xlabel('Models')\n",
    "    axes[1,0].set_ylabel('CV R¬≤ Score')\n",
    "    axes[1,0].set_title('Cross-Validation R¬≤ Score (Mean ¬± Std)')\n",
    "    axes[1,0].set_xticks(x)\n",
    "    axes[1,0].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Overfitting Analysis\n",
    "    overfitting_scores = model_results['Overfitting_Score']\n",
    "    colors = ['red' if score > 0.1 else 'orange' if score > 0.05 else 'green' for score in overfitting_scores]\n",
    "    \n",
    "    axes[1,1].bar(x, overfitting_scores, color=colors, alpha=0.8)\n",
    "    axes[1,1].axhline(y=0.05, color='orange', linestyle='--', alpha=0.7, label='Moderate Overfitting')\n",
    "    axes[1,1].axhline(y=0.1, color='red', linestyle='--', alpha=0.7, label='High Overfitting')\n",
    "    axes[1,1].set_xlabel('Models')\n",
    "    axes[1,1].set_ylabel('Overfitting Score (Train R¬≤ - Test R¬≤)')\n",
    "    axes[1,1].set_title('Overfitting Analysis')\n",
    "    axes[1,1].set_xticks(x)\n",
    "    axes[1,1].set_xticklabels(models, rotation=45, ha='right')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('üîç Advanced Model Performance Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Residual Analysis for Best Model\n",
    "    best_model_idx = model_results['Test_R2'].idxmax()\n",
    "    best_model_name = model_results.loc[best_model_idx, 'Model']\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "    print(f\"üìä Test R¬≤: {model_results.loc[best_model_idx, 'Test_R2']:.4f}\")\n",
    "    print(f\"üìâ Test RMSE: {model_results.loc[best_model_idx, 'Test_RMSE']:.2f}\")\n",
    "    \n",
    "    # Get predictions for best model\n",
    "    best_predictions = model_predictions[best_model_name]\n",
    "    y_test_pred = best_predictions['test_pred']\n",
    "    y_test_actual = best_predictions['test_actual']\n",
    "    \n",
    "    # Calculate residuals\n",
    "    residuals = y_test_actual - y_test_pred\n",
    "    \n",
    "    # Residual Analysis Plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Actual vs Predicted\n",
    "    axes[0,0].scatter(y_test_actual, y_test_pred, alpha=0.6)\n",
    "    axes[0,0].plot([y_test_actual.min(), y_test_actual.max()], \n",
    "                   [y_test_actual.min(), y_test_actual.max()], 'r--', lw=2)\n",
    "    axes[0,0].set_xlabel('Actual Values')\n",
    "    axes[0,0].set_ylabel('Predicted Values')\n",
    "    axes[0,0].set_title('Actual vs Predicted Values')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add R¬≤ annotation\n",
    "    r2_test = r2_score(y_test_actual, y_test_pred)\n",
    "    axes[0,0].text(0.05, 0.95, f'R¬≤ = {r2_test:.4f}', transform=axes[0,0].transAxes,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    # 2. Residuals vs Predicted\n",
    "    axes[0,1].scatter(y_test_pred, residuals, alpha=0.6)\n",
    "    axes[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0,1].set_xlabel('Predicted Values')\n",
    "    axes[0,1].set_ylabel('Residuals')\n",
    "    axes[0,1].set_title('Residuals vs Predicted Values')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Residual Distribution\n",
    "    sns.histplot(residuals, kde=True, ax=axes[0,2])\n",
    "    axes[0,2].axvline(residuals.mean(), color='red', linestyle='--', \n",
    "                      label=f'Mean: {residuals.mean():.2f}')\n",
    "    axes[0,2].set_xlabel('Residuals')\n",
    "    axes[0,2].set_ylabel('Frequency')\n",
    "    axes[0,2].set_title('Distribution of Residuals')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Q-Q Plot for Residuals\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=axes[1,0])\n",
    "    axes[1,0].set_title('Q-Q Plot of Residuals')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Scale-Location Plot\n",
    "    standardized_residuals = np.sqrt(np.abs(residuals / residuals.std()))\n",
    "    axes[1,1].scatter(y_test_pred, standardized_residuals, alpha=0.6)\n",
    "    axes[1,1].set_xlabel('Predicted Values')\n",
    "    axes[1,1].set_ylabel('‚àö|Standardized Residuals|')\n",
    "    axes[1,1].set_title('Scale-Location Plot')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Error Distribution by Prediction Range\n",
    "    pred_ranges = pd.cut(y_test_pred, bins=5, labels=['Low', 'Low-Med', 'Medium', 'Med-High', 'High'])\n",
    "    error_by_range = pd.DataFrame({'Range': pred_ranges, 'Absolute_Error': np.abs(residuals)})\n",
    "    sns.boxplot(data=error_by_range, x='Range', y='Absolute_Error', ax=axes[1,2])\n",
    "    axes[1,2].set_title('Error Distribution by Prediction Range')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'üî¨ Residual Analysis: {best_model_name}', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model_name, residuals\n",
    "\n",
    "# Perform advanced error analysis\n",
    "best_model, residuals = advanced_error_analysis(model_results, model_predictions, data_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Statistical Tests and Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_diagnostics(residuals, X_test, y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Perform comprehensive statistical tests and diagnostics\n",
    "    \"\"\"\n",
    "    print(\"üß™ COMPREHENSIVE MODEL DIAGNOSTICS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    diagnostics_results = {}\n",
    "    \n",
    "    # 1. Normality Tests for Residuals\n",
    "    print(\"\\nüìä NORMALITY TESTS FOR RESIDUALS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Shapiro-Wilk Test\n",
    "    if len(residuals) <= 5000:\n",
    "        shapiro_stat, shapiro_p = shapiro(residuals)\n",
    "        print(f\"Shapiro-Wilk Test:\")\n",
    "        print(f\"  Statistic: {shapiro_stat:.6f}\")\n",
    "        print(f\"  p-value: {shapiro_p:.6f}\")\n",
    "        print(f\"  Normal: {'‚úÖ Yes' if shapiro_p > 0.05 else '‚ùå No'}\")\n",
    "        diagnostics_results['shapiro_p'] = shapiro_p\n",
    "    else:\n",
    "        print(\"Shapiro-Wilk Test: Skipped (sample too large)\")\n",
    "    \n",
    "    # Jarque-Bera Test\n",
    "    jb_stat, jb_p = jarque_bera(residuals)\n",
    "    print(f\"\\nJarque-Bera Test:\")\n",
    "    print(f\"  Statistic: {jb_stat:.6f}\")\n",
    "    print(f\"  p-value: {jb_p:.6f}\")\n",
    "    print(f\"  Normal: {'‚úÖ Yes' if jb_p > 0.05 else '‚ùå No'}\")\n",
    "    diagnostics_results['jb_p'] = jb_p\n",
    "    \n",
    "    # Anderson-Darling Test\n",
    "    ad_stat, ad_critical, ad_significance = anderson(residuals, dist='norm')\n",
    "    print(f\"\\nAnderson-Darling Test:\")\n",
    "    print(f\"  Statistic: {ad_stat:.6f}\")\n",
    "    print(f\"  Critical Values: {ad_critical}\")\n",
    "    print(f\"  Significance Levels: {ad_significance}%\")\n",
    "    normal_ad = ad_stat < ad_critical[2]  # 5% significance level\n",
    "    print(f\"  Normal (5% level): {'‚úÖ Yes' if normal_ad else '‚ùå No'}\")\n",
    "    diagnostics_results['anderson_normal'] = normal_ad\n",
    "    \n",
    "    # 2. Homoscedasticity Tests\n",
    "    print(\"\\nüéØ HOMOSCEDASTICITY TESTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Prepare data for heteroscedasticity tests\n",
    "    X_with_const = sm.add_constant(X_test)\n",
    "    \n",
    "    try:\n",
    "        # Breusch-Pagan Test\n",
    "        bp_stat, bp_p, bp_f_stat, bp_f_p = het_breuschpagan(residuals, X_with_const)\n",
    "        print(f\"Breusch-Pagan Test:\")\n",
    "        print(f\"  LM Statistic: {bp_stat:.6f}\")\n",
    "        print(f\"  p-value: {bp_p:.6f}\")\n",
    "        print(f\"  Homoscedastic: {'‚úÖ Yes' if bp_p > 0.05 else '‚ùå No'}\")\n",
    "        diagnostics_results['bp_p'] = bp_p\n",
    "        \n",
    "        # White Test\n",
    "        white_stat, white_p, white_f_stat, white_f_p = het_white(residuals, X_with_const)\n",
    "        print(f\"\\nWhite Test:\")\n",
    "        print(f\"  LM Statistic: {white_stat:.6f}\")\n",
    "        print(f\"  p-value: {white_p:.6f}\")\n",
    "        print(f\"  Homoscedastic: {'‚úÖ Yes' if white_p > 0.05 else '‚ùå No'}\")\n",
    "        diagnostics_results['white_p'] = white_p\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Heteroscedasticity tests failed: {e}\")\n",
    "    \n",
    "    # 3. Error Distribution Analysis\n",
    "    print(\"\\nüìà ERROR DISTRIBUTION ANALYSIS:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    abs_errors = np.abs(residuals)\n",
    "    squared_errors = residuals ** 2\n",
    "    \n",
    "    print(f\"Mean Absolute Error: {abs_errors.mean():.4f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(abs_errors):.4f}\")\n",
    "    print(f\"90th Percentile Error: {np.percentile(abs_errors, 90):.4f}\")\n",
    "    print(f\"95th Percentile Error: {np.percentile(abs_errors, 95):.4f}\")\n",
    "    print(f\"99th Percentile Error: {np.percentile(abs_errors, 99):.4f}\")\n",
    "    \n",
    "    # Error percentiles\n",
    "    error_percentiles = [50, 75, 90, 95, 99]\n",
    "    print(f\"\\nError Percentiles:\")\n",
    "    for p in error_percentiles:\n",
    "        print(f\"  {p}th percentile: {np.percentile(abs_errors, p):.4f}\")\n",
    "    \n",
    "    # 4. Prediction Intervals\n",
    "    print(\"\\nüéØ PREDICTION INTERVALS:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    residual_std = residuals.std()\n",
    "    \n",
    "    # Calculate prediction intervals (assuming normal distribution)\n",
    "    confidence_levels = [0.68, 0.95, 0.99]\n",
    "    z_scores = [1.0, 1.96, 2.576]\n",
    "    \n",
    "    for conf, z in zip(confidence_levels, z_scores):\n",
    "        interval_width = z * residual_std\n",
    "        coverage = np.mean(np.abs(residuals) <= interval_width) * 100\n",
    "        print(f\"{conf*100:.0f}% Prediction Interval:\")\n",
    "        print(f\"  Width: ¬±{interval_width:.4f}\")\n",
    "        print(f\"  Actual Coverage: {coverage:.1f}%\")\n",
    "        print(f\"  Expected Coverage: {conf*100:.0f}%\")\n",
    "        print()\n",
    "    \n",
    "    # 5. Model Stability Analysis\n",
    "    print(\"üîÑ MODEL STABILITY ANALYSIS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Calculate metrics by prediction quantiles\n",
    "    pred_quantiles = pd.qcut(y_pred, q=5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'])\n",
    "    stability_analysis = pd.DataFrame({\n",
    "        'Quantile': pred_quantiles,\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Residual': residuals,\n",
    "        'Abs_Error': abs_errors\n",
    "    })\n",
    "    \n",
    "    stability_summary = stability_analysis.groupby('Quantile').agg({\n",
    "        'Abs_Error': ['mean', 'std', 'median'],\n",
    "        'Residual': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(\"Performance by Prediction Quantiles:\")\n",
    "    print(stability_summary)\n",
    "    \n",
    "    return diagnostics_results, stability_summary\n",
    "\n",
    "# Get predictions for the best model\n",
    "best_predictions = model_predictions[best_model]\n",
    "y_test_pred = best_predictions['test_pred']\n",
    "y_test_actual = best_predictions['test_actual']\n",
    "\n",
    "# Perform comprehensive diagnostics\n",
    "X_train, X_test, y_train, y_test = data_splits\n",
    "diagnostics, stability = comprehensive_model_diagnostics(residuals, X_test, y_test_actual, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance and Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(X_train, y_train, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using multiple methods\n",
    "    \"\"\"\n",
    "    print(\"üéØ FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Random Forest Feature Importance\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_importance = rf_model.feature_importances_\n",
    "    \n",
    "    # 2. Gradient Boosting Feature Importance\n",
    "    gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    gb_importance = gb_model.feature_importances_\n",
    "    \n",
    "    # 3. Linear Regression Coefficients (absolute values)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(X_train_scaled, y_train)\n",
    "    lr_importance = np.abs(lr_model.coef_)\n",
    "    lr_importance = lr_importance / lr_importance.sum()  # Normalize\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Random_Forest': rf_importance,\n",
    "        'Gradient_Boosting': gb_importance,\n",
    "        'Linear_Regression': lr_importance\n",
    "    })\n",
    "    \n",
    "    # Calculate average importance\n",
    "    importance_df['Average_Importance'] = importance_df[['Random_Forest', 'Gradient_Boosting', 'Linear_Regression']].mean(axis=1)\n",
    "    importance_df = importance_df.sort_values('Average_Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nüìä Feature Importance Rankings:\")\n",
    "    display(importance_df.round(4))\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Random Forest Importance\n",
    "    top_features_rf = importance_df.nlargest(10, 'Random_Forest')\n",
    "    axes[0,0].barh(range(len(top_features_rf)), top_features_rf['Random_Forest'])\n",
    "    axes[0,0].set_yticks(range(len(top_features_rf)))\n",
    "    axes[0,0].set_yticklabels(top_features_rf['Feature'])\n",
    "    axes[0,0].set_title('Random Forest Feature Importance')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient Boosting Importance\n",
    "    top_features_gb = importance_df.nlargest(10, 'Gradient_Boosting')\n",
    "    axes[0,1].barh(range(len(top_features_gb)), top_features_gb['Gradient_Boosting'])\n",
    "    axes[0,1].set_yticks(range(len(top_features_gb)))\n",
    "    axes[0,1].set_yticklabels(top_features_gb['Feature'])\n",
    "    axes[0,1].set_title('Gradient Boosting Feature Importance')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Linear Regression Coefficients\n",
    "    top_features_lr = importance_df.nlargest(10, 'Linear_Regression')\n",
    "    axes[1,0].barh(range(len(top_features_lr)), top_features_lr['Linear_Regression'])\n",
    "    axes[1,0].set_yticks(range(len(top_features_lr)))\n",
    "    axes[1,0].set_yticklabels(top_features_lr['Feature'])\n",
    "    axes[1,0].set_title('Linear Regression Coefficient Importance')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Average Importance\n",
    "    top_features_avg = importance_df.nlargest(10, 'Average_Importance')\n",
    "    axes[1,1].barh(range(len(top_features_avg)), top_features_avg['Average_Importance'])\n",
    "    axes[1,1].set_yticks(range(len(top_features_avg)))\n",
    "    axes[1,1].set_yticklabels(top_features_avg['Feature'])\n",
    "    axes[1,1].set_title('Average Feature Importance')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('üéØ Feature Importance Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance = analyze_feature_importance(data_splits[0], data_splits[2], feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Model Recommendations and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_recommendations(model_results, diagnostics, feature_importance, best_model):\n",
    "    \"\"\"\n",
    "    Generate final recommendations and summary\n",
    "    \"\"\"\n",
    "    print(\"üéØ FINAL MODEL RECOMMENDATIONS & SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Best model summary\n",
    "    best_model_row = model_results[model_results['Model'] == best_model].iloc[0]\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"üìä Test R¬≤: {best_model_row['Test_R2']:.4f}\")\n",
    "    print(f\"üìâ Test RMSE: {best_model_row['Test_RMSE']:.2f}\")\n",
    "    print(f\"üìà Test MAE: {best_model_row['Test_MAE']:.2f}\")\n",
    "    print(f\"üéØ Test MAPE: {best_model_row['Test_MAPE']:.2f}%\")\n",
    "    print(f\"üîÑ CV R¬≤ (Mean¬±Std): {best_model_row['CV_R2_Mean']:.4f}¬±{best_model_row['CV_R2_Std']:.4f}\")\n",
    "    print(f\"‚ö†Ô∏è  Overfitting Score: {best_model_row['Overfitting_Score']:.4f}\")\n",
    "    \n",
    "    # Model ranking\n",
    "    print(\"\\nüìà MODEL RANKING (by Test R¬≤):\")\n",
    "    print(\"-\" * 35)\n",
    "    ranking = model_results.sort_values('Test_R2', ascending=False)[['Model', 'Test_R2', 'Test_RMSE', 'Overfitting_Score']]\n",
    "    for i, (_, row) in enumerate(ranking.iterrows(), 1):\n",
    "        overfitting_status = \"üî¥ High\" if row['Overfitting_Score'] > 0.1 else \"üü° Moderate\" if row['Overfitting_Score'] > 0.05 else \"üü¢ Low\"\n",
    "        print(f\"{i}. {row['Model']}: R¬≤={row['Test_R2']:.4f}, RMSE={row['Test_RMSE']:.2f}, Overfitting={overfitting_status}\")\n",
    "    \n",
    "    # Top features\n",
    "    print(\"\\nüéØ TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "    print(\"-\" * 40)\n",
    "    top_features = feature_importance.head(5)\n",
    "    for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
    "        print(f\"{i}. {row['Feature']}: {row['Average_Importance']:.4f}\")\n",
    "    \n",
    "    # Model diagnostics summary\n",
    "    print(\"\\nüß™ MODEL DIAGNOSTICS SUMMARY:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    if 'jb_p' in diagnostics:\n",
    "        normality_status = \"‚úÖ Normal\" if diagnostics['jb_p'] > 0.05 else \"‚ùå Non-normal\"\n",
    "        print(f\"Residual Normality (Jarque-Bera): {normality_status} (p={diagnostics['jb_p']:.4f})\")\n",
    "    \n",
    "    if 'bp_p' in diagnostics:\n",
    "        homoscedasticity_status = \"‚úÖ Homoscedastic\" if diagnostics['bp_p'] > 0.05 else \"‚ùå Heteroscedastic\"\n",
    "        print(f\"Homoscedasticity (Breusch-Pagan): {homoscedasticity_status} (p={diagnostics['bp_p']:.4f})\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Performance-based recommendations\n",
    "    if best_model_row['Test_R2'] > 0.9:\n",
    "        print(\"‚úÖ Excellent model performance (R¬≤ > 0.9)\")\n",
    "    elif best_model_row['Test_R2'] > 0.8:\n",
    "        print(\"‚úÖ Good model performance (R¬≤ > 0.8)\")\n",
    "    elif best_model_row['Test_R2'] > 0.7:\n",
    "        print(\"‚ö†Ô∏è  Moderate model performance (R¬≤ > 0.7) - consider feature engineering\")\n",
    "    else:\n",
    "        print(\"‚ùå Poor model performance (R¬≤ < 0.7) - significant improvements needed\")\n",
    "    \n",
    "    # Overfitting recommendations\n",
    "    if best_model_row['Overfitting_Score'] > 0.1:\n",
    "        print(\"üî¥ High overfitting detected - consider regularization or more data\")\n",
    "    elif best_model_row['Overfitting_Score'] > 0.05:\n",
    "        print(\"üü° Moderate overfitting - monitor model generalization\")\n",
    "    else:\n",
    "        print(\"üü¢ Low overfitting - good generalization\")\n",
    "    \n",
    "    # Feature recommendations\n",
    "    if len(feature_importance) > 10:\n",
    "        low_importance_features = feature_importance[feature_importance['Average_Importance'] < 0.01]\n",
    "        if len(low_importance_features) > 0:\n",
    "            print(f\"üîß Consider removing {len(low_importance_features)} low-importance features\")\n",
    "    \n",
    "    # Model-specific recommendations\n",
    "    if best_model in ['Random Forest', 'Gradient Boosting']:\n",
    "        print(\"üå≥ Tree-based model selected - good for non-linear relationships\")\n",
    "        print(\"üí° Consider hyperparameter tuning for further improvement\")\n",
    "    elif 'Regression' in best_model:\n",
    "        print(\"üìà Linear model selected - interpretable but may miss non-linear patterns\")\n",
    "        print(\"üí° Consider polynomial features or interaction terms\")\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    print(\"\\nüîß NEXT STEPS FOR IMPROVEMENT:\")\n",
    "    print(\"-\" * 35)\n",
    "    print(\"1. üìä Collect more data if possible\")\n",
    "    print(\"2. üîß Engineer new features based on domain knowledge\")\n",
    "    print(\"3. üéØ Perform hyperparameter tuning\")\n",
    "    print(\"4. üîÑ Try ensemble methods combining multiple models\")\n",
    "    print(\"5. üìà Monitor model performance over time\")\n",
    "    print(\"6. üß™ Validate model on new, unseen data\")\n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_r2': best_model_row['Test_R2'],\n",
    "        'best_rmse': best_model_row['Test_RMSE'],\n",
    "        'overfitting_score': best_model_row['Overfitting_Score'],\n",
    "        'top_features': top_features['Feature'].tolist()\n",
    "    }\n",
    "\n",
    "# Generate final recommendations\n",
    "final_summary = generate_final_recommendations(model_results, diagnostics, feature_importance, best_model)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ ANALYSIS COMPLETE!\")\n",
    "print(\"üìä This comprehensive EDA provides enhanced error analysis and model evaluation.\")\n",
    "print(\"üí° Use the insights above to improve your model further.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results and Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_analysis_results(model_results, feature_importance, diagnostics, final_summary):\n",
    "    \"\"\"\n",
    "    Export analysis results to files\n",
    "    \"\"\"\n",
    "    print(\"üíæ EXPORTING ANALYSIS RESULTS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Export model comparison results\n",
    "        model_results.to_csv('model_comparison_results.csv', index=False)\n",
    "        print(\"‚úÖ Model comparison results exported to 'model_comparison_results.csv'\")\n",
    "        \n",
    "        # Export feature importance\n",
    "        feature_importance.to_csv('feature_importance_analysis.csv', index=False)\n",
    "        print(\"‚úÖ Feature importance analysis exported to 'feature_importance_analysis.csv'\")\n",
    "        \n",
    "        # Export summary report\n",
    "        with open('model_analysis_summary.txt', 'w') as f:\n",
    "            f.write(\"EMPLOYEE COMPENSATION MODEL ANALYSIS SUMMARY\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Best Model: {final_summary['best_model']}\\n\")\n",
    "            f.write(f\"Test R¬≤: {final_summary['best_r2']:.4f}\\n\")\n",
    "            f.write(f\"Test RMSE: {final_summary['best_rmse']:.2f}\\n\")\n",
    "            f.write(f\"Overfitting Score: {final_summary['overfitting_score']:.4f}\\n\\n\")\n",
    "            f.write(\"Top 5 Features:\\n\")\n",
    "            for i, feature in enumerate(final_summary['top_features'], 1):\n",
    "                f.write(f\"{i}. {feature}\\n\")\n",
    "        \n",
    "        print(\"‚úÖ Summary report exported to 'model_analysis_summary.txt'\")\n",
    "        \n",
    "        print(\"\\nüìÅ All results have been exported successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting results: {e}\")\n",
    "\n",
    "# Export results\n",
    "export_analysis_results(model_results, feature_importance, diagnostics, final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}